{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import graphviz \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Stacking\n",
    "from vecstack import stacking\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import pyltr\n",
    "import random as random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(context = 'paper', palette = 'winter_r', style = 'darkgrid', rc= {'figure.facecolor': 'gray',}, font_scale=1.5)\n",
    "\n",
    "\n",
    "def grid(model,parameters):\n",
    "    grid = GridSearchCV(estimator = model, param_grid = parameters, cv = 3, \n",
    "                        scoring = 'roc_auc')\n",
    "    grid.fit(npX,npy_binary.ravel()) #npy\n",
    "    return grid.best_score_, grid.best_estimator_.get_params()\n",
    "\n",
    "def imp_features(model, model_name, params):\n",
    "    Model = model(**params)\n",
    "    Model.fit(npX,npy)\n",
    "    names = X.columns\n",
    "    feature = Model.feature_importances_\n",
    "   \n",
    "    important_features = pd.Series(data = feature, index = names,)\n",
    "    important_features = important_features.sort_values(ascending = True)\n",
    "    return important_features.plot(kind = 'barh', grid = False,title = model_name)\n",
    "\n",
    "def f_selection(model,model_name, params, n):\n",
    "    \n",
    "    Model = model(**params)\n",
    "    Model.fit(npX,npy)\n",
    "    names = X.columns\n",
    "    feature = Model.feature_importances_\n",
    "    #selector = RFECV(Model, step=1, cv=15)\n",
    "    selector = RFE(Model, n, step=1)\n",
    "    selector = selector.fit(npX,npy)\n",
    "    \n",
    "    important_features = pd.Series(data = feature, index = names,)\n",
    "    important_features = important_features.sort_values(ascending = False)\n",
    "    return important_features, names, selector.get_support(indices=True)\n",
    "\n",
    "def testing_confusion_phase(y_test,test):\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    ytest_binary=lb.fit_transform(y_test)\n",
    "\n",
    "\n",
    "# predicted values\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    test_binary=lb.fit_transform(test)\n",
    "\n",
    "\n",
    "    #print('The roc_auc score is:',roc_auc_score(ytest_binary, test_binary))\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(ytest_binary, test_binary)\n",
    "\n",
    "    TP, TN, FP, FN = cm[1,1], cm[0, 0], cm[0,1], cm[1,0]\n",
    "    #print(\"Outcomes on test data: \\n\", \n",
    "     # \"Sensitivity/Recall = {:.3%}\".format(TP/(TP+FN)), '\\n',\n",
    "    # \"Specificity = {:.3%}\".format(TN/(TN+FP)), '\\n',\n",
    "     #\"Accuracy = {:.3%}\".format((TP+TN)/(TP+TN+FP+FN))) # Validated against output of sklearn metrics' accuracy_score class\n",
    "\n",
    "\n",
    "#     sns.heatmap(cm, annot=True, fmt='.0f', xticklabels=[\"Lost\", \"Won\"], \n",
    "#            yticklabels=[\"Lost\", \"Won\"])\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.title('Confusion matrix for test data')\n",
    "#     plt.show()\n",
    "    \n",
    "    Sensitivity_Recall=TP/(TP+FN);\n",
    "    Specificity=TN/(TN+FP)\n",
    "    Accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "    \n",
    "    return Sensitivity_Recall, Specificity, Accuracy\n",
    "    \n",
    "clf_rf = RandomForestClassifier() #RandomForest\n",
    "clf_et = ExtraTreesClassifier() #ExtraTrees\n",
    "clf_bc = BaggingClassifier() #Bagging\n",
    "clf_ada = AdaBoostClassifier() #AdaBoost\n",
    "clf_dt = DecisionTreeClassifier() #DecisionTree\n",
    "clf_xg = XGBClassifier() #XGBoost\n",
    "clf_lr = LogisticRegression() #LogisticRegression\n",
    "clf_svm = SVC() #SVM\n",
    "clf_nnet = MLPClassifier() # nnet\n",
    "\n",
    "model_rf = RandomForestClassifier #RandomForest\n",
    "model_et = ExtraTreesClassifier #ExtraTrees\n",
    "model_bc = BaggingClassifier #Bagging\n",
    "model_ada = AdaBoostClassifier #AdaBoost\n",
    "model_dt = DecisionTreeClassifier #DecisionTree\n",
    "model_xg = XGBClassifier #XGBoost\n",
    "model_lr = LogisticRegression #LogisticRegression\n",
    "model_svm = SVC #SVM\n",
    "model_nnet = MLPClassifier # nnet\n",
    "\n",
    "# initial parameters for each model\n",
    "\n",
    "parameters_xg = {\n",
    "        'learning_rate': [0.05,0.1],\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1],\n",
    "        'subsample': [0.6, 0.8],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4],\n",
    "        'n_estimators': [400],  \n",
    "        'min_child_weight': [4], \n",
    "        'reg_alpha': [6,0], \n",
    "        'reg_lambda': [1,3,8],\n",
    "        'max_delta_step':[2],\n",
    "        'nthread':[-1], # to run at all cpu threads\n",
    "        }\n",
    "\n",
    "\n",
    "parameters_rf = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [ 90, 100],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [8, 10],\n",
    "    'n_estimators': [ 200, 300],\n",
    "    \n",
    "}\n",
    "\n",
    "parameters_bc = {\n",
    "    'n_estimators': np.arange(10, 101, 10), \n",
    "}\n",
    "\n",
    "parameters_ada = {\n",
    "              'n_estimators': [25, 50, 100, 150, 250] ,\n",
    "              'learning_rate':[0.01,0.025,0.05,0.1],\n",
    "             }\n",
    "\n",
    "\n",
    "parameters_et=  {\n",
    "     'n_estimators': [500,700,750],\n",
    "     #'max_features': [25,50,75], \n",
    "     'criterion': ['entropy','gini'],\n",
    "     'min_samples_split':[5,10,15],\n",
    "     'max_depth':[50,75,100],\n",
    "     'min_samples_leaf':[5,10,15],\n",
    " }\n",
    "\n",
    "parameters_nnet=  {\n",
    " 'activation': ['relu', 'logistic', 'tanh'],\n",
    " 'alpha': [0.0001,0.005],\n",
    " 'hidden_layer_sizes': [(50,),(25,2)],\n",
    " 'learning_rate_init': [0.001,0.025,0.05],\n",
    " 'max_iter': [700,750],\n",
    " 'random_state': [None,1],\n",
    " 'solver': ['adam','lbfgs'], #'sgd'\n",
    "\n",
    " }\n",
    "\n",
    "parameters_svm = {'kernel':('linear', 'rbf'), \n",
    "              'C':(1,0.25,0.5,0.75),\n",
    "              'gamma': (1,2,3,'auto'),\n",
    "              'decision_function_shape':('ovo','ovr'),\n",
    "              'shrinking':(True,False),\n",
    "              'probability':(True,True)}\n",
    "\n",
    "parameters_lr = {'C': np.arange(1e-05, 10, 0.2), \n",
    "                 'penalty': ('l2','l1'),\n",
    "                 'warm_start':(True,False)\n",
    "                }\n",
    "\n",
    "parameters_dt = {'class_weight':[None,None], \n",
    "                  'criterion':['gini','gini'], \n",
    "                  'max_depth':[None,None],\n",
    "                  'max_features':[None, None], \n",
    "                  'max_leaf_nodes':[None,None],\n",
    "                  'min_impurity_split':[1e-07, 1e-07],\n",
    "                  'min_samples_leaf':[1,1],\n",
    "                  'min_samples_split':[2,2] ,\n",
    "                  'min_weight_fraction_leaf':[0.0,0.0],\n",
    "                  'presort':[False,False],\n",
    "                  'random_state':[None, None],\n",
    "                  'splitter':['best','best']}\n",
    "\n",
    "\n",
    "def single_performance(df, model, clf, parameters, ncv, n_reps, nsplt):\n",
    "    \n",
    "    Sensitivity_Recall = np.zeros(n_reps)\n",
    "    Specificity=  np.zeros(n_reps)\n",
    "    Accuracy =  np.zeros(n_reps)\n",
    "\n",
    "    for u in range(n_reps):\n",
    "\n",
    "        df=df.sample(frac=1) #shuffle the data set\n",
    "        train, test = train_test_split(df, test_size=1/nsplt)\n",
    "\n",
    "\n",
    "        y_train = train['FlgQuotationStatus_Final']\n",
    "        X_train = train.drop(['FlgQuotationStatus_Final','Customer','Creation_Date'], axis = 1)\n",
    "        y_test = test['FlgQuotationStatus_Final']\n",
    "        X_test = test.drop(['FlgQuotationStatus_Final','Customer','Creation_Date'], axis = 1)\n",
    "        trainNames=list(X_train)\n",
    "\n",
    "      \n",
    "            \n",
    "# balance the data\n",
    "        smote = SMOTE(ratio='minority')\n",
    "        X_train, y_train= smote.fit_sample(X_train, y_train)\n",
    "                     \n",
    "            \n",
    "            #\n",
    "            \n",
    "        scaler = StandardScaler()  \n",
    "            # Don't cheat - fit only on training data\n",
    "        scaler.fit(X_train)  \n",
    "\n",
    "        X_train = scaler.transform(X_train)  \n",
    "            # apply same transformation to test data\n",
    "        X_test = scaler.transform(X_test)  \n",
    "\n",
    "\n",
    "        npX = np.array(X_train).copy()\n",
    "        npy = np.array(y_train).copy()\n",
    "    \n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        npy_binary=lb.fit_transform(npy)\n",
    "            ##\n",
    "            \n",
    "            # cv for tuning hyper parameters\n",
    "        grid = GridSearchCV(estimator = clf, param_grid = parameters, cv = ncv, \n",
    "                        scoring = 'roc_auc')\n",
    "        grid.fit(npX,npy_binary.ravel()) #npy\n",
    "            \n",
    "            \n",
    "            \n",
    "        best_score=grid.best_score_\n",
    "        best_params=grid.best_estimator_.get_params()\n",
    "\n",
    "            #train model\n",
    "        clf = model(**best_params)\n",
    "\n",
    "        clf.fit(X_train,y_train)\n",
    "        test = clf.predict(X_test)\n",
    "        test_prob = clf.predict_proba(X_test)\n",
    "            \n",
    "        \n",
    "        SR, SC, AC= testing_confusion_phase(y_test,test)    \n",
    "    \n",
    "\n",
    "        #record scores\n",
    "        Sensitivity_Recall[u] = SR\n",
    "        Specificity[u]= SC\n",
    "        Accuracy[u]  = AC\n",
    "        \n",
    "    return Sensitivity_Recall, Specificity, Accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "greece = pd.read_csv('greece.csv')\n",
    "df = pd.read_csv('outputdata.csv',encoding='latin1')\n",
    "greece['dgreece']= 100*greece['Greece_price'].diff()/greece['Greece_price']\n",
    "df = df.merge(greece,on = 'Quotation_Date', how = 'left')\n",
    "df = df.dropna()\n",
    "df['TranspCosts_per_Kg']=abs(df['TranspCosts_per_Kg'])\n",
    "count_ratio = df.groupby('Customer').agg({'FlgQuotationStatus_Final': lambda x: len(x[x=='Won'])/len(x)}).rename(columns={'FlgQuotationStatus_Final':'Quotation_won_ratio'})\n",
    "count_ratio.reset_index(level=0, inplace=True)\n",
    "df = df.merge(count_ratio, how = 'outer', on = 'Customer')\n",
    "df['Creation_Date'] = pd.to_datetime(df['Creation_Date'])\n",
    "df = df.drop(['Customer_Country','Repackers','Quotation_Date'], axis = 1)\n",
    "df = pd.get_dummies(df, columns = ['Cheese_Type','Segment','Volatility','AgeGroup','Ingredient_Applications'])\n",
    "df_opt = (df.loc[(df.Customer == 'Frischpack GmbH')& (df.Cheese_Type_GOUDA == 1)][-1:]).copy()\n",
    "df['PR_greece'] = df['SP_2']/df['Greece_price_x']\n",
    "df.sort_values(['Customer', 'Creation_Date'], inplace=True)\n",
    "df['date_diff'] = df.groupby(['Customer'])[['Creation_Date']].transform(lambda x: x.diff()).fillna(0)\n",
    "df[['date_diff']]=df['date_diff'].dt.days\n",
    "df[['dVolume','dValue','dSales_Price','dGreece_price','dPR_greece','dSP_2','dTransport']] = df.groupby(['Customer','Cheese_Type_GOUDA'], as_index= False)['Volume','Value','Sales_Price','Greece_price_x','PR_greece','SP_2','TranspCosts_per_Kg'].transform(lambda x: (x.diff()).fillna(0))\n",
    "\n",
    "df=df.drop(['Greece_price_y'],axis=1)\n",
    "\n",
    "# # OUTLIER\n",
    "# df_outlier=df.loc[(df['Sales_Price']>=1.94)&(df['Sales_Price']<=3.93)]\n",
    "# df=df_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Production_Balance'] = 0\n",
    "df['Sales']=0\n",
    "df['month_year'] = df['Creation_Date'].dt.to_period('M')\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-01').to_period('M'), 'Production_Balance'] = 350\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-02').to_period('M'), 'Production_Balance'] = 850\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-03').to_period('M'), 'Production_Balance'] = 2000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-04').to_period('M'), 'Production_Balance'] = 900\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-05').to_period('M'), 'Production_Balance'] = 900\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-06').to_period('M'), 'Production_Balance'] = 100\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-07').to_period('M'), 'Production_Balance'] = 250\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-08').to_period('M'), 'Production_Balance'] = -900\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-09').to_period('M'), 'Production_Balance'] = -1000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-10').to_period('M'), 'Production_Balance'] = 200\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-11').to_period('M'), 'Production_Balance'] = 400\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-12').to_period('M'), 'Production_Balance'] = 1700\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-01').to_period('M'), 'Production_Balance'] = 800\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-02').to_period('M'), 'Production_Balance'] = 2200\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-03').to_period('M'), 'Production_Balance'] = 1400\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-04').to_period('M'), 'Production_Balance'] = -400\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-05').to_period('M'), 'Production_Balance'] = -500\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-06').to_period('M'), 'Production_Balance'] = -400\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-07').to_period('M'), 'Production_Balance'] = -350\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-08').to_period('M'), 'Production_Balance'] = 800\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-09').to_period('M'), 'Production_Balance'] = 1500\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-10').to_period('M'), 'Production_Balance'] = 250\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-11').to_period('M'), 'Production_Balance'] = 1000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-12').to_period('M'), 'Production_Balance'] = 1600\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-01').to_period('M'), 'Production_Balance'] = -700\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-02').to_period('M'), 'Production_Balance'] = -100\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-03').to_period('M'), 'Production_Balance'] = -850\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-04').to_period('M'), 'Production_Balance'] = 300\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-05').to_period('M'), 'Production_Balance'] = -1000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-06').to_period('M'), 'Production_Balance'] = -700\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-07').to_period('M'), 'Production_Balance'] = -500\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-08').to_period('M'), 'Production_Balance'] = -2800\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-09').to_period('M'), 'Production_Balance'] = -1000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-10').to_period('M'), 'Production_Balance'] = 600\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-11').to_period('M'), 'Production_Balance'] = 2000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-12').to_period('M'), 'Production_Balance'] = 2100\n",
    "\n",
    "\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-01').to_period('M'), 'Sales'] = 18150.233887\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-02').to_period('M'), 'Sales'] = 19924.3405\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-03').to_period('M'), 'Sales'] = 22020.457165\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-04').to_period('M'), 'Sales'] = 21589.246198\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-05').to_period('M'), 'Sales'] = 21706.505662\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-06').to_period('M'), 'Sales'] = 21149.638548\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-07').to_period('M'), 'Sales'] = 18265.543153\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-08').to_period('M'), 'Sales'] = 19766.190484\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-09').to_period('M'), 'Sales'] = 20048.347281\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-10').to_period('M'), 'Sales'] = 18063.732871\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-11').to_period('M'), 'Sales'] = 18144.777343\n",
    "df.loc[df['month_year'] == pd.to_datetime('2016-12').to_period('M'), 'Sales'] = 16037.076585\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-01').to_period('M'), 'Sales'] = 18244\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-02').to_period('M'), 'Sales'] = 16738\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-03').to_period('M'), 'Sales'] = 20334\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-04').to_period('M'), 'Sales'] = 19096\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-05').to_period('M'), 'Sales'] = 20945\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-06').to_period('M'), 'Sales'] = 19724\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-07').to_period('M'), 'Sales'] = 16010\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-08').to_period('M'), 'Sales'] = 17632\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-09').to_period('M'), 'Sales'] = 16388\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-10').to_period('M'), 'Sales'] = 16415\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-11').to_period('M'), 'Sales'] = 18058\n",
    "df.loc[df['month_year'] == pd.to_datetime('2017-12').to_period('M'), 'Sales'] = 16053\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-01').to_period('M'), 'Sales'] = 19007.721823\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-02').to_period('M'), 'Sales'] = 19181.119919\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-03').to_period('M'), 'Sales'] = 19787.444863\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-04').to_period('M'), 'Sales'] = 16884.283094\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-05').to_period('M'), 'Sales'] = 19501.825713\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-06').to_period('M'), 'Sales'] = 18237.525663\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-07').to_period('M'), 'Sales'] = 16935.858738\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-08').to_period('M'), 'Sales'] = 20871.050881\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-09').to_period('M'), 'Sales'] = 18716.998901\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-10').to_period('M'), 'Sales'] = 18376.869341\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-11').to_period('M'), 'Sales'] = 19000\n",
    "df.loc[df['month_year'] == pd.to_datetime('2018-12').to_period('M'), 'Sales'] = 19300\n",
    "\n",
    "df = df.drop(['month_year'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(df)\n",
    "#df=df.drop(['AgeGroup_>2','AgeGroup_0-2','Cheese_Type_GOUDA','Cheese_Type_EDAM','Ingredient_Applications_0','Ingredient_Applications_1'], axis = 1)\n",
    "\n",
    "# positive 1 correlation\n",
    "df=df.drop(['SP_2','dgreece_y', 'dSP_2','Segment_Ingredient Applications'], axis = 1)\n",
    "# negative (-1) correlation\n",
    "df=df.drop(['Cheese_Type_EDAM','Volatility_High','AgeGroup_>2','Ingredient_Applications_0'], axis = 1)\n",
    "\n",
    "df=df.drop(['date_diff',\n",
    " 'dVolume',\n",
    " 'dValue',\n",
    " 'dSales_Price',\n",
    " 'dGreece_price',\n",
    " 'dPR_greece',\n",
    " 'dTransport','dgreece_x'], axis = 1)\n",
    "\n",
    "df=df.drop(['Cheese_Type_GOUDA',\n",
    "            'Ingredient_Applications_1',\n",
    "            'Segment_Repackers',\n",
    "            'Volatility_Low',\n",
    "            'Segment_Industrial Food Applications',\n",
    "            'CustomerAge',\n",
    "            'AgeGroup_0-2',\n",
    "            'Segment_0',\n",
    "            'Segment_Resellers/C&C/Wholesale',\n",
    "           'STDEVAVG',\n",
    "           'Production_Balance',\n",
    "           'Sales',\n",
    "           'Greece_price_x',\n",
    "           'PR_greece'], axis=1)\n",
    "\n",
    "# y=df['FlgQuotationStatus_Final']\n",
    "# df =df.drop(['FlgQuotationStatus_Final','Customer','Creation_Date'], axis = 1)\n",
    "# X=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer',\n",
       " 'TranspCosts_per_Kg',\n",
       " 'Creation_Date',\n",
       " 'Volume',\n",
       " 'Value',\n",
       " 'Sales_Price',\n",
       " 'FlgQuotationStatus_Final',\n",
       " 'Quotation_won_ratio']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a single classifier \n",
    "\n",
    "## rf: RandomForestClassifier\n",
    "\n",
    "## et: ExtraTreesClassifier\n",
    "\n",
    "## bc: BaggingClassifier\n",
    "\n",
    "## ada: AdaBoostClassifier\n",
    "\n",
    "## dt: DecisionTreeClassifier\n",
    "\n",
    "## xg: XGBClassifier\n",
    "\n",
    "## lr: LogisticRegression\n",
    "\n",
    "## svm: SVC\n",
    "\n",
    "## nnet: MLPClassifier\n",
    "\n",
    "\n",
    "### EXAMPLE: clf=clf_bc, model= model_bc, parameters= parameters_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Sensitivity/Recall over  75  repetitions is:  0.938713586231 ± 0.000351258999275\n",
      "The average Specificity over  75  repetitions is:  0.837026331062 ± 0.00200639833899\n",
      "The average Accuracy over 75  repetitions is:  0.912876052948 ± 0.000223238787015\n"
     ]
    }
   ],
   "source": [
    "# Bagging classifier\n",
    "\n",
    "ncv=4\n",
    "n_reps=75\n",
    "nsplt=4\n",
    "model=model_bc\n",
    "clf=clf_bc\n",
    "parameters = parameters_bc\n",
    "\n",
    "Sensitivity_Recall, Specificity, Accuracy= single_performance(df, model, clf, parameters,\n",
    "                                                               ncv, n_reps, nsplt)\n",
    "\n",
    "print('The average Sensitivity/Recall over ', n_reps ,' repetitions is: ', np.mean(Sensitivity_Recall),'±',np.var(Sensitivity_Recall))\n",
    "print('The average Specificity over ', n_reps ,' repetitions is: ', np.mean(Specificity),'±',np.var(Specificity))\n",
    "print('The average Accuracy over', n_reps ,' repetitions is: ', np.mean(Accuracy),'±',np.var(Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "\n",
    "\n",
    "def ensemble_performance(df, models, clfs, parameters, \n",
    "                         ncv, n_reps, nsplt,out_model, \n",
    "                         out_clf, out_parameters):\n",
    "    \n",
    "    Sensitivity_Recall = np.zeros(n_reps)\n",
    "    Specificity=  np.zeros(n_reps)\n",
    "    Accuracy =  np.zeros(n_reps)\n",
    "\n",
    "    for u in range(n_reps):\n",
    "\n",
    "            df=df.sample(frac=1) #shuffle the data set\n",
    "            train, test = train_test_split(df, test_size=1/nsplt)\n",
    "\n",
    "\n",
    "            y_train = train['FlgQuotationStatus_Final']\n",
    "            X_train = train.drop(['FlgQuotationStatus_Final','Customer','Creation_Date'], axis = 1)\n",
    "            y_test = test['FlgQuotationStatus_Final']\n",
    "            X_test = test.drop(['FlgQuotationStatus_Final','Customer','Creation_Date'], axis = 1)\n",
    "            trainNames=list(X_train)\n",
    "\n",
    "      \n",
    "            \n",
    "# balance the data\n",
    "            smote = SMOTE(ratio='minority')\n",
    "            X_train, y_train= smote.fit_sample(X_train, y_train)\n",
    "                     \n",
    "            \n",
    "            #\n",
    "            \n",
    "            scaler = StandardScaler()  \n",
    "            # Don't cheat - fit only on training data\n",
    "            scaler.fit(X_train)  \n",
    "\n",
    "            X_train = scaler.transform(X_train)  \n",
    "            # apply same transformation to test data\n",
    "            X_test = scaler.transform(X_test)  \n",
    "\n",
    "\n",
    "            npX = np.array(X_train).copy()\n",
    "            npy = np.array(y_train).copy()\n",
    "    \n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            npy_binary=lb.fit_transform(npy)\n",
    "            ##\n",
    "            \n",
    "            best_params=[]\n",
    "            models_1=[]\n",
    "            \n",
    "            for i in range(len(models)):\n",
    "                    # cv for tuning hyper parameters\n",
    "                    grid = GridSearchCV(estimator = clfs[i], param_grid = parameters[i], cv = ncv, \n",
    "                        scoring = 'roc_auc')\n",
    "                    grid.fit(npX,npy_binary.ravel()) #npy\n",
    "            \n",
    "            \n",
    "            \n",
    "                    #best_score=grid.best_score_\n",
    "                    best_params.append(grid.best_estimator_.get_params())\n",
    "                    \n",
    "                    models_1.append(models[i](**best_params[i]))\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "            S_train_1, S_test_1 = stacking(models_1,                   # list of models\n",
    "                               X_train, y_train, X_test,   # data\n",
    "                               \n",
    "                               regression=False,           # classification task (if you need \n",
    "                                                           #     regression - set to True)\n",
    "                               mode='oof_pred',            # mode: oof for train set, fit on full \n",
    "                                                           #     train and predict test set once\n",
    "                               needs_proba=True,           # predict probabilities (if you need \n",
    "                                                           #     class labels - set to False) \n",
    "                               #save_dir='.',               # save result and log in current dir \n",
    "                                                           #     (to disable saving - set to None)\n",
    "                               metric=roc_auc_score,            # metric: callable log_loss\n",
    "                               n_folds=10,                  # number of folds\n",
    "                               stratified=True,            # stratified split for folds\n",
    "                               shuffle=True,               # shuffle the data\n",
    "                               random_state=0,             # ensure reproducibility\n",
    "                               verbose=False)                  # print all info\n",
    "\n",
    "\n",
    "             # Run this before the Ensembles below\n",
    "             # apply gridsearch on new data\n",
    "\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            y_train_binary=lb.fit_transform(y_train)\n",
    "\n",
    "            \n",
    "            grid = GridSearchCV(estimator = out_clf, param_grid = out_parameters, cv = ncv, \n",
    "                        scoring = 'roc_auc')\n",
    "            grid.fit(S_train_1,y_train_binary.ravel()) #npy        \n",
    "        \n",
    "            out_best_params=grid.best_estimator_.get_params()\n",
    "            out = out_model(**out_best_params)\n",
    "\n",
    "\n",
    "\n",
    "            out.fit(S_train_1,y_train)\n",
    "            test = out.predict(S_test_1)\n",
    "            test_prob = out.predict_proba(S_test_1)\n",
    "\n",
    "            \n",
    "        \n",
    "            SR, SC, AC= testing_confusion_phase(y_test,test)    \n",
    "    \n",
    "\n",
    "        #record scores\n",
    "            Sensitivity_Recall[u] = SR\n",
    "            Specificity[u]= SC\n",
    "            Accuracy[u]  = AC\n",
    "        \n",
    "    return Sensitivity_Recall, Specificity, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Sensitivity/Recall over  2  repetitions is:  0.959525939177 ± 0.000296468025256\n",
      "The average Specificity over  2  repetitions is:  0.801776531089 ± 2.1856389967e-05\n",
      "The average Accuracy over 2  repetitions is:  0.92238267148 ± 0.000263915859714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ncv=3\n",
    "n_reps=2\n",
    "nsplt=4\n",
    "\n",
    "#input models\n",
    "models=[model_bc, model_lr, model_rf, model_ada, model_svm]\n",
    "clfs=[clf_bc, clf_lr, clf_rf, clf_ada, clf_svm]\n",
    "parameters = [parameters_bc, parameters_lr, parameters_rf, parameters_ada, parameters_svm]\n",
    "\n",
    "#output_models\n",
    "\n",
    "out_model=model_bc\n",
    "out_clf=clf_bc\n",
    "out_parameters=parameters_bc\n",
    "\n",
    "\n",
    "Sensitivity_Recall, Specificity, Accuracy= ensemble_performance(df, models, clfs, parameters,\n",
    "                                                               ncv, n_reps, nsplt,out_model, \n",
    "                                                               out_clf, out_parameters)\n",
    "\n",
    "print('The average Sensitivity/Recall over ', n_reps ,' repetitions is: ', np.mean(Sensitivity_Recall),'±',np.var(Sensitivity_Recall))\n",
    "print('The average Specificity over ', n_reps ,' repetitions is: ', np.mean(Specificity),'±',np.var(Specificity))\n",
    "print('The average Accuracy over', n_reps ,' repetitions is: ', np.mean(Accuracy),'±',np.var(Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
